{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center>\n",
    "<h1>Homework Assignment</h1>\n",
    "<br>\n",
    "<h2>Predictive Analytics using Python (CIS432)</h2>\n",
    "<h3>Simon Business School</h3>\n",
    "<h3>Due date: 3/6/2018 23:55</h3>\n",
    "</center> \n",
    "\n",
    "1. Each student should submit an individual homework assignment.\n",
    "2. Discussing solutions with others is allowed.\n",
    "3. Copying solutions is prohibited.\n",
    "4. Late submissions will not be accepted.\n",
    "5. Write your solutions in Jupyter IPython notebooks. Use markdown cells to format your document. Your solutions should be replicable, as it will be executed on the grader's computer.\n",
    "6. Solution files should be uploaded to Blackboard.\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read Chapter 10 in __An Introduction to Statistical Learning with Applications in R__ by Gareth James, Daniela Witten, Trevor Hastie and Robert Tibshirani ([link](http://www-bcf.usc.edu/~gareth/ISL/ISLR%20Seventh%20Printing.pdf)) and answer the following questions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Student Name: Weixuezi WANG"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 1: Clustering\n",
    "    \n",
    "1. Discuss a typical use-case of KMeans\n",
    "1. Explain in high-level how KMeans works\n",
    "2. Explain the following input parameters for the KMeans algorithm (see [SK-Learn documentaiton on KMeans](http://scikit-learn.org/stable/modules/generated/sklearn.cluster.KMeans.html)): \n",
    "    - n\\_clusters\n",
    "    - init\n",
    "    - n\\_init\n",
    "    - max\\_iter\n",
    "    - tol    \n",
    "3. Explain the output of the KMeans algorithm (see [SK-Learn documentaiton on KMeans](http://scikit-learn.org/stable/modules/generated/sklearn.cluster.KMeans.html)): \n",
    "    - cluster\\_centers\\_\n",
    "    - labels\\_ \n",
    "    - inertia\\_ \n",
    "   \n",
    "   What are the dimensions (e.g., shape in the case of matrixes or lengths in the case of vectors) of each output variable? \n",
    "   \n",
    "__Note__: you may (and should) use SK-Learn documentation ([Link](http://scikit-learn.org/stable/modules/generated/sklearn.cluster.KMeans.html)), but do not copy-paste the explanations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q1: Discuss a typical use-case of KMeans\n",
    "\n",
    "In the marketing field, K-means clustering can help company to execute marketing segmentatoin. For example, we can obtain n observations correspond to customers and p features of them. The features can be a large amount of measurements, such as age, the distance from nearest urban area, occupation, educatoin, median household income, marital status and so on. According to KMeans clustering algorithm, we can identify several subgroups of customer. The customers who are in the same cluster should have similar features and may show the similar preference to the marketing strategy. In this way, we can reach them with different and customized advertisements or particular products."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q2: Explain in high-level how KMeans works\n",
    "1. First of all, we should set the desirable number of cluster K. Then the algorithm will assign each observation randomly to exactly on cluster. At this step, there are two properties should be satisfied. The one is that each observation should belong to at least one of the K cluster. The other is no observation belong to more than one cluster. That is there is no overlapping for all the clusters.\n",
    "\n",
    "2. if the $C_1,C_2.....C_k$ denote sets containing the indices of the observations in each cluster. Then we want to calculate the W($C_k$) which means the amount that the observations within a cluster differ from each other.\n",
    "<img src=\"image/Formula_1.PNG\" style=\"width: 20%;\" />\n",
    "That is, we want to partition the observations into K clusters and achieve the sum of within-cluster variation over all K clusters as small as possible. To make it actionable, we use squared Euclidean distance to define the within-cluster variation, that is:\n",
    "<img src=\"image/Formula_2.PNG\" style=\"width: 30%;\" />\n",
    "Combining the above two formula, we can get the optimization problem that defines K-means clustering:\n",
    "<img src=\"image/Formula_3.PNG\" style=\"width: 30%;\" />\n",
    "\n",
    "3. To achieve the above goal with the practicle method for a large size of observations. We iterate assign observation until the cluster assignments stop changing. To be detailed, firstly, we conpute the centroid for each K clusters. The centroid is the vector of the p featur means for all the instance in the same cluster.\n",
    "\n",
    "4. Then with the new set of K cluster centroid, we reallocate each observation to the cluster from which the Euclidean distance is smaller or closest. The clustering obtained will continually improve until the result no longer changes.\n",
    "\n",
    "5. Because the K-means algorithm finds a local rather than a global optimum point, the result we get may partly depend on the initial centroid we choose. Therefore, it is essential to run the Kmean algorithm multiple times to find the best solution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q3: Explain the following input parameters for the KMeans algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- n_clusters:\n",
    "    \n",
    "    This parameter decides how many clusters will generate, that is the number of the centroid to be formed. It is optional or should be integer, and the default value is 8.\n",
    "    \n",
    "- init:\n",
    "\n",
    "    It is method to initialize the centriod and should be in the format of {‘k-means++’, ‘random’ or an ndarray}. If you choose 'k-means++', the algorithm will choose the center or clustering in a smart way and speed up to convergence. When we pass 'random', the intialized center will be the k instance randomly picked from dataset. While, an ndarray is in the shape of (n_clusters, n_features) and gives the center as argument states.\n",
    "    \n",
    "- n_init:\n",
    "\n",
    "    It is in the form of integer and default as 10. It indicate the number of times K-mean will run with different centriod seed. The final reult is the best one of n_init consecutive you specified which run in terms of inertia.\n",
    "    \n",
    "- max_iter:\n",
    "\n",
    "    This parameter specify the the maximum number of iteration for a single run in this algorithm. The default value is 300 and you can pass any integer you want in to it.\n",
    "    \n",
    "- tol:\n",
    "\n",
    "    tol has default value as 1e-4 and the format should be float number. It means the relative tolerance with regards to inertia to declare convergence."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q4: Explain the output of the KMeans algorithm\n",
    "- cluster\\_centers\\_\n",
    "\n",
    "    It will be shown in an array with the form of [n_clusters, n_features]. It coordinates of the cluster centers.\n",
    "    \n",
    "- labels\\_\n",
    "\n",
    "    It indicate the labels of each point in the cluster.\n",
    "    \n",
    "- inertia\\_\n",
    "\n",
    "    This output is in the form of float. It explains the sum of the distance of the obserbations to their closest cluster center."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 2: PCA\n",
    "\n",
    "1. Discuss a typical use-case of PCA\n",
    "2. How would you choose the number of components? \n",
    "3. Consider the following code: \n",
    "\n",
    "    from sklearn.decomposition import PCA<br>\n",
    "    pca = PCA(n_components = 150)<br>\n",
    "    X_reduced = pca.fit_transform(X_train)<br>\n",
    "    X_recovered = pca.inverse_transform(X_reduced)<br>\n",
    "\n",
    "    Note: X_train is a matrix of size 60,000 by 784\n",
    "    \n",
    "    1. What are the dimensions of X_reduced and X_recovered?\n",
    "    2. Explain the content of X_reduced and X_recovered?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q1: Discuss a typical use-case of PCA\n",
    "\n",
    "The typucal use of PCA is to find a low-dimensional representation of a dataset that contains as much as possible of the variation. For instance, when we collect the data from the survey, we will have many features or variables. They may contains similar information and are not useful to be included for analytic work. Moreover, the algorithm with too many features will have high variation and may not be an accurate model. Also, it is hard to obtain a straightforward visualization. Using a PCA we can now identify what are the most important dimensions and just keep a few of them to explain most of the variance and information we see in out data. Moreover, it will also enable us to identify what the most important variables in the original feature space are. Then we can analyse dataset more effeciently and accurately."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q2: How would you choose the number of components?\n",
    "\n",
    "From the technique side, if we have n observations with p features, we can find min(n − 1, p) distinct principal components. However, it is not necessary to use all of them. In most case, we just use the first few pricipal component in order to visualize and better interpret the data. We usually visualize the data and choose the number of principal components by a scree plot (The example of the plot is shown below):\n",
    "<img src=\"image/Plot_1.PNG\" style=\"width: 30%;\" />\n",
    "Typically, we choose the number of PC that are required to explain the variation in the dataset. This is done by looking for the point in scree plot which the proportion of variance explained by each subsequent principal component drops off. We often define it as an elbow in the graph. For example, in the figure above, we can demonstrate that large amount of variation can be explained by first two principle component and the elbow is at the third point. Because from the third PC, the percentage of variance which is explained by PC drops dramatically and only have less than 10 percent of information. Therefore, in this case, we will choose 2 PC."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q3_A: What are the dimensions of X_reduced and X_recovered?\n",
    "dimensions of  X_reduced:150\n",
    "\n",
    "Edimensions of  X_recovered:784"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q3_B: Explain the content of X_reduced and X_recovered?\n",
    "\n",
    "X_reduced is computed from pca.fit_transform(). It fit the model with X_train and apply the dimensionality reduction on X_train. The content of it is an array, shaped as (n_samples, n_components), showing each observation's eigenvector in each new PCA component after fitting 150 components.\n",
    "\n",
    "X_recovered is gotten from pca.inverse_transform(). It will transform data back to its original space. The content of it is a array like shape (n_samples, n_features). In other words, return original dataset whose transform would be X_train and we will find each oberservation's performance in initial feature."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 3: PageRank (Optional)\n",
    "Based on Chapter 11 in \"The analytics edge\" by Bertsimas et al. ([link](https://www.dynamic-ideas.com/books/kgsni67q285zmpdit17ix1cwbnuqac))\n",
    "\n",
    "The file  \"votes.csv\" contains the voting preferences of an executive board on the new CEO. Use the PageRank algorithm to rank the candidates according to the votes. You can think about each person as a node, and each vote as an arc from one person to another.\n",
    "1. After how many iterations did the algorithm converge? explain.\n",
    "2. Plot a graph representing the voting preferences (who votes for who).\n",
    "3. Rank the candidates (display the result of PageRank).\n",
    "4. Rank the candidates based on the total number of votes each candidate received.\n",
    "5. Explain the differences in the final rankings created by the two methods."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 4: simple recommendation system (optional)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this assignment, you will construct a recommendation system that predicts movie ratings. \n",
    "\n",
    "1. The file ratings.dat contains movie ratings from the [MovieLens](http://grouplens.org/datasets/movielens/1m/) database. The four columns contain information about user id, movie id, rating, and the time of rating. Load the file to memory.\n",
    "2. __Create a subset of the data which only contains the movies and users with the largest number of ratings. Use the top 10 users and movies (top 10 in terms of the maximal number of ratings).__\n",
    "2. Plot histograms of the subset data presenting \n",
    "    - the frequency of ratings across all users (that is, how many votes are for rating 1, rating 2, etc.)\n",
    "    - the frequency of ratings by user (the number of ratings per user)\n",
    "    - the frequency of ratings by movie (the number of ratings per movie)    \n",
    "3. Split the subset data randomly to training and validation sets.\n",
    "4. Compare the performance of the four baseline classifiers discussed in class, item-item collaborative filtering and user-user collaborative filtering algorithms.\n",
    "5. __You may use any similarity measure of your choice__, including pearson correlation implemented by the function DataFrame.corr.\n",
    "7. Run the code again, this time with the top 200 users and movies and report on your findings (this may take a few minutes).\n",
    "\n",
    "Make sure to comment your code.\n",
    "\n",
    "Remark: in this exercise you will be implementing the system, although in practice, you may want to consider an existing libary/framework such as [SurPRISE](http://surpriselib.com/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1::1193::5::978300760\n",
      "1::661::3::978302109\n",
      "1::914::3::978301968\n",
      "1::3408::4::978300275\n",
      "1::2355::5::978824291\n",
      "1::1197::3::978302268\n",
      "1::1287::5::978302039\n",
      "1::2804::5::978300719\n",
      "1::594::4::978302268\n",
      "1::919::4::978301368\n"
     ]
    }
   ],
   "source": [
    "!head -10 ratings.dat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 1000209  1000209 24594131 ratings.dat\n"
     ]
    }
   ],
   "source": [
    "!wc ratings.dat"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
